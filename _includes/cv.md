I am a Reasearch Scientist at Meta AI working on Human-Centric Multimodal Machine Learning and Generative Modeling. Prior to that I completed my PhD at the Language Technologies Institute at <b>Carnegie Mellon University</b> where I was advised by [Dr. Louis-Philippe Morency (LP)](https://www.cs.cmu.edu/~morency/) in the [Multicomp Lab](http://multicomp.cs.cmu.edu/). My research focused on endowing agents and remote avatars with <b>Social Intelligence</b> by means of <b>Multimodal Learning</b>. One of the use-cases where we extensively apply these technologies is <b>Computer Animation</b>. These directions have the potential of making a meaningful impact on remote communication, collaborations, education and mental health for human-human and human-robot interaction, especially now when a lot of social and work spaces are gradually moving online. <br> <br> In the past, I have also interned at Facebook Reality Labs on generation of nonverbal behaviours for a communicating avatar. As an undergraduate researcher at <b>Indian Institute of Technology(IIT), Kanpur</b>, I worked with [Dr. Rajesh Hegde](http://home.iitk.ac.in/~rhegde/) on <b>Spatial Audio</b> and <b>Speaker Diarization</b>, and [Dr. Vinay Namboodiri](https://www.cse.iitk.ac.in/users/vinaypn/) on <b>Video Summarization</b>


## <i class="fa fa-chevron-right"></i> News
<table class="table table-hover">
<tr>
  <td class='col-md-3'>Feb 2023</td>
  <td>Survey paper on Co-Speech Gestures accepted in the STAR track at Eurographics 2023. <a href="https://arxiv.org/abs/2301.05339" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a></td>
</tr>
<tr>
  <td class='col-md-3'>May 2022</td>
  <td>Excited to join Meta AI as a Research Scientist</td>
</tr>
<tr>
  <td class='col-md-3'>April 2022</td>
  <td>Defended my PhD dissertation on <b>Communication Beyond Words: Grounding Visual Body Motion with Language</b> <a href="https://chahuja.com/files/chaitanya_ahuja_phd_thesis.pdf" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a></td>
</tr>
<tr>
  <td class='col-md-3'>April 2022</td>
  <td>Humbled to be a Highlighted Reviewer at ICLR 2022</td>
</tr>
<tr>
  <td class='col-md-3'>March 2022</td>
  <td>Paper on Low-Resource Adaptation of Spatio-Temporal Crossmodal Generative Models accepted at CVPR 2022</td>
</tr>
</table>

<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo" onclick="change()" id="more">More</button>
<div id="demo" class="collapse">
<table class="table table-hover">
      <tr>
	    <td class='col-md-3'>May 2020</td>
	    <td>We are organizing the <b>First Workshop on Crossmodal Social Animation</b> at <a href="http://iccv2021.thecvf.com/">ICCV2021</a>. Consider submissing your work. <a href="http://sites.google.com/view/xs-anim" target="_blank"><button type="button" class="btn btn-success">webpage</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>December 2020</td>
	    <td>Succesfully proposed my thesis titled <b>Communication Beyond Words: Grounding Visual Body Motion with Language</b> <a href="https://drive.google.com/open?id=1lrk5J4vJjBirAyZMOK6pbozGkB4DKebO&authuser=cahuja%40andrew.cmu.edu&usp=drive_fs" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>September 2020</td>
	    <td>Paper on Co-Speech Gesture Generation from Language accepted at Findings at EMNLP 2020</td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>September 2020</td>
	    <td>Paper on Impact of Personality on Non-verbal behvaiours accepted at IVA 2020</td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>August 2020</td>
	    <td>PATS (Pose-Audio-Transcripts-Style) Dataset released. <a href="http://chahuja.com/pats" target="_blank"><button type="button" class="btn btn-success">webpage</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>August 2020</td>
	    <td>Code for Style Transfer for Co-Speech Gesture Animation released. <a href="https://github.com/chahuja/mix-stage" target="_blank"><button type="button" class="btn btn-info">code</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>July 2020</td>
	    <td>Paper on Style Transfer for Co-Speech Gesture Animation accepted at ECCV 2020</td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>August 2019</td>
	    <td>Paper on Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations accepted at ICMI 2019. <a href="https://arxiv.org/pdf/1910.02181.pdf" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>August 2019</td>
	    <td>Honourable mention in LTI SRS symposium on my talk on Natural Language Grounded Pose Forecasting</td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>July 2019</td>
	    <td>Paper on Natural Language Grounded Pose Forecasting accepted at 3DV 2019 <a href="https://arxiv.org/pdf/1907.01108.pdf" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a> <a href="http://chahuja.com/language2pose" target="_blank"><button type="button" class="btn btn-success">webpage</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>March 2018</td>
	    <td>Excited to work at Facebook Reality Labs in Summer'18</td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>January 2018</td>
	    <td>Paper on Lattice Recurrent Units accepted at AAAI 2018 <a href="https://arxiv.org/abs/1710.02254" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a> <a href="http://chahuja.com/lru" target="_blank"><button type="button" class="btn btn-success">webpage</button></a></td>
	  </tr>	  
      <tr>
	    <td class='col-md-3'>October 2017</td>
	    <td>Our survey on Multimodal Machine Learning is online <a href="https://arxiv.org/pdf/1705.09406.pdf" target="_blank"><button type="button" class="btn btn-primary">pdf</button></a></td>
	  </tr>	  
</table>
</div>


## <i class="fa fa-chevron-right"></i> Book Chapters

<table class="table table-hover">

<tr>
<td class="col-md-3"><a href='https://dl.acm.org/doi/abs/10.1145/3107990.3107993' target='_blank'><img src="pics/publications/baltruvsaitis2018challenges.png"/></a> </td>
<td>
    <strong>1. Challenges and applications in multimodal machine learning</strong><br>
    T. Baltrusaitis, <strong>C. Ahuja</strong>, and L. Morency<br>
    The Handbook of Multimodal-Multisensor Interfaces 2018<br>
    
     <a href='https://dl.acm.org/doi/abs/10.1145/3107990.3107993' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Pre-prints

<table class="table table-hover">

<tr>
<td class="col-md-3"><img src="pics/publications/ahuja2023continual.png"/></td>
<td>
    <strong>2. Continual Learning for Personalized Co-speech Gesture Generation</strong><br>
    <strong>C. Ahuja</strong>, P. Joshi, R. Ishii, and L. Morency<br>
    Preprint 2023<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2023continual").toggle()'><button type='button' class='btn btn-danger'>abs</button></a><br>
    
<div id="abs_ahuja2023continual" style="text-align: justify; display: none" markdown="1">
Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/pdf/2208.08080.pdf' target='_blank'><img src="pics/publications/lee2022multimodal.png"/></a> </td>
<td>
    <strong>1. Multimodal Lecture Presentations Dataset: Understanding Multimodality in Educational Slides</strong><br>
    D. Lee, <strong>C. Ahuja</strong>, P. Liang, S. Natu, and L. Morency<br>
    Preprint 2022<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_lee2022multimodal").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/pdf/2208.08080.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='https://github.com/dondongwon/MLPDataset' target='_blank'><button type='button' class='btn btn-info'>code</button></a> <br>
    
<div id="abs_lee2022multimodal" style="text-align: justify; display: none" markdown="1">
Lecture slide presentations, a sequence of pages that contain text and figures accompanied by speech, are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing AI to aid in student learning as intelligent teacher assistants, we introduce the Multimodal Lecture Presentations dataset as a large-scale benchmark testing the capabilities of machine learning models in multimodal understanding of educational content. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce two research tasks which are designed as stepping stones towards AI agents that can explain (automatically captioning a lecture presentation) and illustrate (synthesizing visual figures to accompany spoken explanations) educational content. We provide manual annotations to help implement these two research tasks and evaluate stateof-the-art models on them. Comparing baselines and human student performances, we find that current models struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. Towards addressing this issue, we also introduce PolyViLT, a multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentations.
</div>

</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Selected Publications

<a href="https://scholar.google.com/citations?user=CX8zqPoAAAAJ&hl" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a>

<table class="table table-hover">

<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2301.05339' target='_blank'><img src="pics/publications/nyatsanga2023communication.png"/></a> </td>
<td>
    <strong>12. A Comprehensive Review of Data-Driven Co-Speech Gesture Generation</strong><br>
    S. Nyatsanga, T. Kucherenko, <strong>C. Ahuja</strong>, G. Henter, and M. Neff<br>
    EUROGRAPHICS 2023<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_nyatsanga2023communication").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/abs/2301.05339' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_nyatsanga2023communication" style="text-align: justify; display: none" markdown="1">
Gestures that accompany speech are an essential part of natural and efficient embodied human communication. The automatic generation of such co-speech gestures is a long-standing problem in computer animation and is considered an enabling technology in film, games, virtual social spaces, and for interaction with social robots. The problem is made challenging by the idiosyncratic and non-periodic nature of human co-speech gesture motion, and by the great diversity of communicative functions that gestures encompass. Gesture generation has seen surging interest recently, owing to the emergence of more and larger datasets of human gesture motion, combined with strides in deep-learning-based generative models, that benefit from the growing availability of data. This review article summarizes co-speech gesture generation research, with a particular focus on deep generative models. First, we articulate the theory describing human gesticulation and how it complements speech. Next, we briefly discuss rule-based and classical statistical gesture synthesis, before delving into deep learning approaches. We employ the choice of input modalities as an organizing principle, examining systems that generate gestures from audio, text, and non-linguistic input. We also chronicle the evolution of the related training data sets in terms of size, diversity, motion quality, and collection method. Finally, we identify key research challenges in gesture generation, including data availability and quality; producing human-like motion; grounding the gesture in the co-occurring speech in interaction with other speakers, and in the environment; performing gesture evaluation; and integration of gesture synthesis into applications. We highlight recent approaches to tackling the various key challenges, as well as the limitations of these approaches, and point toward areas of future development.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://chahuja.com/files/chaitanya_ahuja_phd_thesis.pdf' target='_blank'><img src="pics/publications/ahuja2022communication.png"/></a> </td>
<td>
    <strong>11. Communication Beyond Words: Grounding Visual Body Motion with Language</strong><br>
    <strong>C. Ahuja</strong><br>
    PhD dissertation, Carnegie Mellon University 2022<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2022communication").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://chahuja.com/files/chaitanya_ahuja_phd_thesis.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_ahuja2022communication" style="text-align: justify; display: none" markdown="1">
The central theme of this thesis is to understand the two-way relationship (a.k.a. grounding) between human body motions and its associated spoken language, which includes both verbal and vocal cues. Understanding this complex relationship will help us to both better understand the meaning intended by body gestures and provide us with the knowledge necessary to generate more realistic nonverbal body animations with interactive technologies. With these motivations in mind, we propose three key challenges: (1) Nonverbal Grounding as the core component of this thesis to study the close relationship between spoken language and motion, (2) Personalization to better understand idiosyncrasies and commonalities on how people gesture, and (3) Low Resource Learning when gestures occur infrequently or the amount of labeled data is limited and often unbalanced. These challenges investigate the commonalities, uniqueness and generalizability of visual body communication respectively in the presence of verbal and vocal information
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ahuja_Low-Resource_Adaptation_for_Personalized_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf' target='_blank'><img src="pics/publications/ahuja2021low.png"/></a> </td>
<td>
    <strong>10. Low-Resource Adaptation of Spatio-Temporal Crossmodal Generative Models</strong><br>
    <strong>C. Ahuja</strong>, D. Lee, and L. Morency<br>
    CVPR 2022<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2021low").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ahuja_Low-Resource_Adaptation_for_Personalized_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='https://chahuja.com/diffgan' target='_blank'><button type='button' class='btn btn-info'>code</button></a> <br>
    
<div id="abs_ahuja2021low" style="text-align: justify; display: none" markdown="1">
Personalizing an avatar for co-speech gesture generation from spoken language requires learning the idiosyncrasies of a person’s gesture style from a small amount of data. Previous methods in gesture generation require large amounts of data for each speaker, which is often infeasible. We propose an approach, named DiffGAN, that efficiently personalizes co-speech gesture generation models of a high-resource source speaker to target speaker with just 2 minutes of target training data. A unique characteristic of DiffGAN is its ability to account for the crossmodal grounding shift, while also addressing the distribution shift in the output domain. We substantiate the effectiveness of our approach a large scale publicly available dataset through quantitative, qualitative and user studies, which show that our proposed methodology significantly outperforms prior approaches for low-resource adaptation of gesture generation. Code and videos can be found at https://chahuja.com/diffgan.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf' target='_blank'><img src="pics/publications/ahuja2020no.png"/></a> </td>
<td>
    <strong>9. No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</strong><br>
    <strong>C. Ahuja</strong>, D. Lee, R. Ishii, and L. Morency<br>
    EMNLP Findings 2020<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2020no").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='https://github.com/chahuja/aisle' target='_blank'><button type='button' class='btn btn-info'>code</button></a> <br>
    
<div id="abs_ahuja2020no" style="text-align: justify; display: none" markdown="1">
We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and audio. We introduce Adversarial Importance Sampled Learning, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous stateof-the-art approaches for gesture generation.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://dl.acm.org/doi/10.1145/3383652.3423908' target='_blank'><img src="pics/publications/ishii2020impact.png"/></a> </td>
<td>
    <strong>8. Impact of Personality on Nonverbal Behavior Generation</strong><br>
    R. Ishii, <strong>C. Ahuja</strong>, Y. Nakano, and L. Morency<br>
    IVA 2020<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ishii2020impact").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://dl.acm.org/doi/10.1145/3383652.3423908' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_ishii2020impact" style="text-align: justify; display: none" markdown="1">
To realize natural-looking virtual agents, one key technical challenge is to automatically generate nonverbal behaviors from spoken language. Since nonverbal behavior varies depending on personality, it is important to generate these nonverbal behaviors to match the expected personality of a virtual agent. In this work, we study how personality traits relate to the process of generating individual nonverbal behaviors from the whole body, including the head, eye gaze, arms, and posture. To study this, we first created a dialogue corpus including transcripts, a broad range of labelled nonverbal behaviors, and the Big Five personality scores of participants in dyad interactions. We constructed models that can predict each nonverbal behavior label given as an input language representation from the participants’ spoken sentences. Our experimental results show that personality can help improve the prediction of nonverbal behaviors.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2007.12553' target='_blank'><img src="pics/publications/ahuja2020style.png"/></a> </td>
<td>
    <strong>7. Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional Mixture Approach</strong><br>
    <strong>C. Ahuja</strong>, D. Lee, Y. Nakano, and L. Morency<br>
    ECCV 2020<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2020style").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/abs/2007.12553' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='http://chahuja.com/mix-stage' target='_blank'><button type='button' class='btn btn-success'>webpage</button></a> <br>
    
<div id="abs_ahuja2020style" style="text-align: justify; display: none" markdown="1">
How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a specific speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent 'A' in the gesturing style of a target speaker 'B'. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker's gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model significantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. Link to code, data, and videos: http://chahuja.com/mix-stage
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/pdf/1910.02181.pdf' target='_blank'><img src="pics/publications/ahuja2019to.png"/></a> </td>
<td>
    <strong>6. To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations</strong><br>
    <strong>C. Ahuja</strong>, S. Ma, L. Morency, and Y. Sheikh<br>
    ICMI 2019<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2019to").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/pdf/1910.02181.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_ahuja2019to" style="text-align: justify; display: none" markdown="1">
Non verbal behaviours such as gestures, facial expressions, body posture, and para-linguistic cues have been shown to complement or clarify verbal messages. Hence to improve telepresence, in form of an avatar, it is important to model these behaviours, especially in dyadic interactions. Creating such personalized avatars not only requires to model intrapersonal dynamics between a avatar's speech and their body pose, but it also needs to model interpersonal dynamics with the interlocutor present in the conversation. In this paper, we introduce a neural architecture named Dyadic Residual-Attention Model (DRAM), which integrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using selective attention to generate sequences of body pose conditioned on audio and body pose of the interlocutor and audio of the human operating the avatar. We evaluate our proposed model on dyadic conversational data consisting of pose and audio of both participants, confirming the importance of adaptive attention between monadic and dyadic dynamics when predicting avatar pose. We also conduct a user study to analyze judgments of human observers. Our results confirm that the generated body pose is more natural, models intrapersonal dynamics and interpersonal dynamics better than non-adaptive monadic/dyadic models.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/pdf/1907.01108.pdf' target='_blank'><img src="pics/publications/ahuja2019language.png"/></a> </td>
<td>
    <strong>5. Language2Pose: Natural Language Grounded Pose Forecasting</strong><br>
    <strong>C. Ahuja</strong> and L. Morency<br>
    3DV 2019<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2019language").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/pdf/1907.01108.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='https://github.com/chahuja/language2pose' target='_blank'><button type='button' class='btn btn-info'>code</button></a>  <a href='http://chahuja.com/language2pose' target='_blank'><button type='button' class='btn btn-success'>webpage</button></a> <br>
    
<div id="abs_ahuja2019language" style="text-align: justify; display: none" markdown="1">
Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations. In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language-toPose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-toend using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and humanannotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/pdf/1710.02254.pdf' target='_blank'><img src="pics/publications/ahuja2018lattice.png"/></a> </td>
<td>
    <strong>4. Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling</strong><br>
    <strong>C. Ahuja</strong> and L. Morency<br>
    AAAI 2018<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2018lattice").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/pdf/1710.02254.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a>  <a href='https://github.com/chahuja/lru' target='_blank'><button type='button' class='btn btn-info'>code</button></a>  <a href='http://chahuja.com/lru' target='_blank'><button type='button' class='btn btn-success'>webpage</button></a> <br>
    
<div id="abs_ahuja2018lattice" style="text-align: justify; display: none" markdown="1">
Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family of new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/1705.09406' target='_blank'><img src="pics/publications/baltruvsaitis2017multimodal.png"/></a> </td>
<td>
    <strong>3. Multimodal Machine Learning: A Survey and Taxonomy</strong><br>
    T. Baltrusaitis, <strong>C. Ahuja</strong>, and L. Morency<br>
    TPAMI 2017<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_baltruvsaitis2017multimodal").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='https://arxiv.org/abs/1705.09406' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_baltruvsaitis2017multimodal" style="text-align: justify; display: none" markdown="1">
—Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='files/icassp_chahuja_paper.pdf' target='_blank'><img src="pics/publications/ahuja2014fast.png"/></a> </td>
<td>
    <strong>2. Fast modelling of pinna spectral notches from HRTFs using linear prediction residual cepstrum</strong><br>
    <strong>C. Ahuja</strong> and R. Hegde<br>
    ICASSP 2014<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_ahuja2014fast").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='files/icassp_chahuja_paper.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_ahuja2014fast" style="text-align: justify; display: none" markdown="1">
Developing individualized head related transfer functions (HRTF) is an essential requirement for accurate virtualization of sound. However it is time consuming and complicated for both the subject and the developer. Obtaining the spectral notches which are the most prominent features of HRTF is very important to reconstruct the head related impulse response (HRIR) accurately. In this paper, a method suitable for fast computation of the frequencies of spectral notches is proposed. The linear prediction residual cepstrum is used to compute the spectral notches with a high degree of accuracy in this work. Subsequent use of Batteaus Reflection model to overlay the spectral notches on the pinna images indicate that the proposed method is able to provide finer contours. Experiments on reconstruction of the HRIR indicates that the method performs better than other methods.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='files/hscma_chahuja_paper.pdf' target='_blank'><img src="pics/publications/sohni2014extraction.png"/></a> </td>
<td>
    <strong>1. Extraction of pinna spectral notches in the median plane of a virtual spherical microphone array</strong><br>
    A. Sohni, <strong>C. Ahuja</strong>, and R. Hegde<br>
    HSCMA 2014<br>
    
     
<a href='javascript: none'
    onclick='$("#abs_sohni2014extraction").toggle()'><button type='button' class='btn btn-danger'>abs</button></a> <a href='files/hscma_chahuja_paper.pdf' target='_blank'><button type='button' class='btn btn-primary'>pdf</button></a> <br>
    
<div id="abs_sohni2014extraction" style="text-align: justify; display: none" markdown="1">
In this paper, a fast method for the extraction of pinna spectral notches (PSN) in the median plane of a virtual spherical microphone array is discussed. In general, PSN can be extracted from the Head Related Impulse Response (HRIR) measured by a spherical array of microphones. However, the PSN extracted herein are computationally complex and also not accurate at lower elevation angles. This work proposes a novel approach to reconstruct the HRIR using microphones over the median plane of a virtual spherical array. The virtual spherical array itself is simulated using the Fourier Bessel series (FBS). Subsequently, these HRIRs are used to extract the PSN. This method is computationally efficient since it is done over the median plane rather than over the complete sphere. On the other hand, it is also accurate due to the utilization of the Fourier Bessel series in the extraction of the PSN. Experimental results obtained on the CIPIC database indicate a high degree of resemblance to the actual pinna walls, even at the lower elevation angles. The results are motivating enough for the method to be considered for resolving elevation ambiguity in 3D audio.
</div>

</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Resources

<table class="table table-hover">

<tr>
<td class="col-md-3"><img src="pics/publications/ahuja2020pats.png"/></td>
<td>
    <strong>1. PATS Dataset: Pose, Audio, Transcripts and Style</strong><br>
    <strong>C. Ahuja</strong>, D. Lee, Y. Nakano, and L. Morency<br>
     <br>
    
     <a href='https://chahuja.com/pats' target='_blank'><button type='button' class='btn btn-success'>webpage</button></a> <br>
    
</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
        <strong>Ph.D. in Language Technologies</strong>
          (4.02/4.00)
        <br>
      Carnegie Mellon University | Pittsburgh, PA
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Thesis: <a href='https://chahuja.com/files/chaitanya_ahuja_phd_thesis.pdf'>Communication Beyond Words: Grounding Visual Body Motion with Language</a>
        <br> Advisor: <a href='https://www.cs.cmu.edu/~morency/'>Louis-Philippe Morency</a>
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>2015 - 2022</td>
  </tr>
  <tr>
    <td>
        <strong>B.Tech. in Electrical Engineering</strong>
          (9.5/10)
        <br>
      Indian Institute of Technology | Kanpur, India
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Advisors: <a href='http://home.iitk.ac.in/~rhegde/'>Rajesh Hegde</a>, <a href='https://vinaypn.github.io/'>Vinay P. Namboodiri</a>
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>2011 - 2015</td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Academic Talks
<table class="table table-hover">
<tr>
  <td>
        <b>Communication Beyond Words: Grounding Visual Body Motion with Spoken Language</b>
	
      <br> KTH Stockholm, Online
  </td>
  <td class='col-md-2' style='text-align:right;'>April 2021</td>
</tr>
<tr>
  <td>
        <b><a href="https://slideslive.com/38940175/no-gestures-left-behind-learning-relationships-between-spoken-language-and-freeform-gestures">Learning Relationships between Spoken Language and Freeform Gestures</a></b>
	
      <br> EMNLP 2020 Workshop on NLP Beyond Text, Online
  </td>
  <td class='col-md-2' style='text-align:right;'>Novermber 2020</td>
</tr>
<tr>
  <td>
        <b><a href="https://www.youtube.com/embed/L7ZGHmMJLCc">Style Transfer for Co-speech Gesture Generation</a></b>
  </td>
  <td class='col-md-2' style='text-align:right;'>September 2020</td>
</tr>
<tr>
  <td>
        <b>End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations</b>
	
      <br> ACM International Conference on Multimodal Interaction, Suzhou, China
  </td>
  <td class='col-md-2' style='text-align:right;'>October 2019</td>
</tr>
<tr>
  <td>
        <b>Natural Language Grounded Pose Forecasting</b>
	
      <br> LTI Student Research Symposium, Pittsburgh PA
  </td>
  <td class='col-md-2' style='text-align:right;'>August 2019</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Student Mentorship
<table class="table table-hover">
<tr>
  <td>
        <a href="https://www.linkedin.com/in/don-dong-won-lee-ab964b172/">Dong Won Lee</a> (CMU BS → CMU MS in Machine Learning): Self-supervised generative models.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        <a href="https://web.iiit.ac.in/~shradha.sehgal/">Shradha Sehgal</a> (IIIT Hyderabad B.Tech.): Evaluation of generative models.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        Arvin Wu (CMU BS): Social intelligence benchmarking.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        <a href="https://www.linkedin.com/in/nikithamurikinati/">Nikitha Murikinati</a> (CMU BS): Study of relationships between co-speech gestures and prosody.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        <a href="https://www.linkedin.com/in/sharathrao1/">Sharath Rao</a> (CMU MS → PlayStation) Back-channel prediction in dyadic conversations.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        Qingtao Hu (CMU MS → Amazon): Unsupervised disentanglement of style and content in images.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
<tr>
  <td>
        Anirudha Rayasam (CMU MS → Google): Language grounded pose forecasting.
  </td>
  <td class='col-md-1' style='text-align:right;'></td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Teaching Experience
<table class="table table-hover">
<tr>
<td><strong><a href="https://structuredprediction11763.github.io/structuredprediction.github.io/">Structured Prediction for Language and Other Discrete Data</a></strong> (CMU 11-763), Head TA</td>
<td class='col-md-2' style='text-align:right;'>Spring 2018</td>
</tr>
<tr>
<td><strong>Multimodal Machine Learning</strong> (CMU 11-777), Head TA</td>
<td class='col-md-2' style='text-align:right;'>Spring 2017</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Professional Activities and Service
<table class="table table-hover">
<tr>
  <td>
     <a href="https://sites.google.com/view/xs-anim">Co-organizer: ICCV 2021 First Workshop on Crossmodal Social Animation</a>
  <td class='col-md-2' style='text-align:right;'>2021</td>
  </td>
</tr>
<tr>
  <td>
      Co-organizer: Multimodal Machine Learning Reading Group, CMU
  <td class='col-md-2' style='text-align:right;'>Spring 2020</td>
  </td>
</tr>
<tr>
  <td>
      Conference Program Commiitee: Neurips, SIGGRAPH, ICLR, ACL, EMNLP, ACM Multimedia, ICMI
  <td class='col-md-2' style='text-align:right;'></td>
  </td>
</tr>
<tr>
  <td>
      Workshop Program Committee: NeurIPS workshop on Multimodal Machine Learning, ACL Workshop on Multimodal Language, NAACL-HLT Student Research Workshop, ICMI GENEA Workshop
  <td class='col-md-2' style='text-align:right;'></td>
  </td>
</tr>
<tr>
  <td>
      Grant Reviewer: Army Research Office (ARO)
  <td class='col-md-2' style='text-align:right;'></td>
  </td>
</tr>
<tr>
  <td>
      CMU Graduate Applicant Support Program Volunteer
  <td class='col-md-2' style='text-align:right;'>2020</td>
  </td>
</tr>
<tr>
  <td>
      CMU AI Undergraduate Research Mentor
  <td class='col-md-2' style='text-align:right;'>2020-21</td>
  </td>
</tr>
<tr>
  <td>
      CMU Graduate Student Association Representative for Language Technologies Institute
  <td class='col-md-2' style='text-align:right;'>2017</td>
  </td>
</tr>
</table>
